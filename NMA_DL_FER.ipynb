{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alaeimo/NMA_DL_FER/blob/Zahra/NMA_DL_FER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Facial Emotion Recognision\n",
        "## Nueromatch Academy- Deep Learning course\n",
        "## Summer 2023\n",
        "## Pod name: Bellusaurus_line\n",
        "## Group 2\n",
        "- Mohammad ([email](mailto:alaeimono@gmail.com) | [GitHub](https://github.com/alaeimo) | [linkedin](https://linkedin.com/in/...) )\n",
        "- Zahra ([email](mailto:zs.noori@gmail.com) | [GitHub](https://github.com/zsnoori) | [linkedin](https://linkedin.com/in/zsnoori) )\n",
        "- Rishabh ([email](mailto:rishabhbapat@gmail.com) | [GitHub](https://github.com/...) | [linkedin](https://linkedin.com/in/...) )\n",
        "- Vahid ([email](mailto:m1vahid@gmail.com) | [GitHub](https://github.com/...) | [linkedin](https://linkedin.com/in/...) )"
      ],
      "metadata": {
        "id": "K6Lp8S_Hti2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the requierments"
      ],
      "metadata": {
        "id": "d0VW0P6R6tsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install dependencies\n",
        "%%capture\n",
        "!pip install torchinfo"
      ],
      "metadata": {
        "id": "GUswGh6PaSQf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xSIbMFTZ-eGp"
      },
      "outputs": [],
      "source": [
        "# @title Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchinfo import summary\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO02iVGLM9lz",
        "outputId": "e301961a-24f5-4c8e-9fba-fed45b240ad8",
        "cellView": "form"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set random seed\n",
        "\n",
        "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
        "\n",
        "# for DL its critical to set the random seed so that students can have a\n",
        "# baseline to compare their results to expected results.\n",
        "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
        "\n",
        "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=None, seed_torch=True):\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "# In case that `DataLoader` is used\n",
        "def seed_worker(worker_id):\n",
        "  worker_seed = torch.initial_seed() % 2**32\n",
        "  np.random.seed(worker_seed)\n",
        "  random.seed(worker_seed)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r3Dz8cD6PsH0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set log file path\n",
        "# Specify the directory path in Google Drive\n",
        "log_directory = '/content/drive/MyDrive/Colab Notebooks/Logs'\n",
        "model_directory = os.path.join(log_directory,\"Models\")\n",
        "# Check if the directory exists, create it if not\n",
        "if not os.path.exists(log_directory):\n",
        "    os.makedirs(log_directory)\n",
        "    print(\"Directory created:\", log_directory)\n",
        "else:\n",
        "    print(\"Directory already exists:\", log_directory)\n",
        "if not os.path.exists(model_directory):\n",
        "    os.makedirs(model_directory)\n",
        "    print(\"Directory created:\", model_directory)\n",
        "else:\n",
        "    print(\"Directory already exists:\", model_directory)\n",
        "log_file_path = os.path.join(log_directory,\"log_models.csv\")\n",
        "print(\"log file path:\", log_file_path)\n",
        "print(\"models path:\", model_directory)"
      ],
      "metadata": {
        "id": "DLWdOY31_jU1",
        "outputId": "71554eb9-73b4-4b04-9a3a-23b47208fd58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory created: /content/drive/MyDrive/Colab Notebooks/Logs\n",
            "Directory created: /content/drive/MyDrive/Colab Notebooks/Logs/Models\n",
            "log file path: /content/drive/MyDrive/Colab Notebooks/Logs/log_models.csv\n",
            "models path: /content/drive/MyDrive/Colab Notebooks/Logs/Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set device (GPU or CPU)\n",
        "\n",
        "# inform the user if the notebook uses GPU or CPU.\n",
        "\n",
        "def set_device():\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  if device != \"cuda\":\n",
        "    print(\"WARNING: For this notebook to perform best, \"\n",
        "        \"if possible, in the menu under `Runtime` -> \"\n",
        "        \"`Change runtime type.`  select `GPU` \")\n",
        "  else:\n",
        "    print(\"GPU is enabled in this notebook.\")\n",
        "\n",
        "\n",
        "  return device"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-LRoXzmlPz-d"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(seed=2021)\n",
        "device = set_device()\n",
        "use_cuda = True if device=='GPU' else False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYBJrLLiP2yr",
        "outputId": "c1078719-695d-4212-b3fd-0aad96d5730b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seed 2021 has been set.\n",
            "WARNING: For this notebook to perform best, if possible, in the menu under `Runtime` -> `Change runtime type.`  select `GPU` \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(use_cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE6u4sEymg_H",
        "outputId": "bbe384b0-9d6d-4f7c-c471-9f69a48e46a4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "8NBdwlnmsva0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Sad\", 5:\"Surprise\", 6:\"Neutral\"}\n",
        "emotions_inv = {v:k for k,v in emotions.items()}"
      ],
      "metadata": {
        "id": "GaMmpz6gORsz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset class\n",
        "# @markdown FER2013 dataset includes 3 types of samples, Training, PublicTest, and PrivateTest. This class loads data of each part.\n",
        "\n",
        "class FER2013(Dataset):\n",
        "    def __init__(self, csv_file:str, split:str='Training', transforms:transforms.Compose=None):\n",
        "      \"\"\"\n",
        "      Arguments:\n",
        "          csv_file (string): Path to the FER2013 icml_face_data.csv file with annotations.\n",
        "          split = 'Training' for train , 'PublicTest' for validation, 'PrivateTest' for test\n",
        "          transform (callable, optional): Optional transform to be applied on a sample.\n",
        "      \"\"\"\n",
        "      self._main_df = pd.read_csv(csv_file)\n",
        "      self.split = split\n",
        "      self.transforms = transforms\n",
        "      self._samples = self._load()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self._samples[idx]\n",
        "        img = torch.as_tensor(img).float()\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        return img, label\n",
        "\n",
        "    def _load(self):\n",
        "      df = self._main_df\n",
        "      df.columns = df.columns.str.replace(' ', '')\n",
        "      df = df[df['Usage'] == self.split].copy()\n",
        "      samples = []\n",
        "      for index, row in df.iterrows():\n",
        "          label = row['emotion']\n",
        "          img_arr = row['pixels'].split(' ')\n",
        "          img_arr = np.array(img_arr).astype(int)\n",
        "          img_arr = img_arr.reshape(48,48)\n",
        "          samples.append((img_arr, label))\n",
        "      samples = np.array(samples, dtype=object)\n",
        "      return samples"
      ],
      "metadata": {
        "id": "xs4udahwuqdf",
        "cellView": "form"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define batch size and dataset path\n",
        "batch_size = 32\n",
        "dataset_csv_file = \"./drive/MyDrive/Colab Notebooks/datasets/fer2013/icml_face_data.csv\""
      ],
      "metadata": {
        "id": "TOPKCbIjuqgY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Transformations\n",
        "torchvision_transforms = True  # True/False if you want use torchvision augmentations\n",
        "# Define the transformations to apply\n",
        "transforms_train = transforms.Compose([transforms.ToPILImage()])\n",
        "if torchvision_transforms:\n",
        "    transforms_train.transforms.extend([\n",
        "                transforms.RandomRotation(10),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.RandomCrop(size=48, padding=4)])\n",
        "\n",
        "transforms_train.transforms.extend([\n",
        "                transforms.Grayscale(num_output_channels=3),\n",
        "                transforms.ToTensor()])\n",
        "\n",
        "transforms_validation = transforms.Compose([transforms.ToPILImage(),\n",
        "                                            transforms.Grayscale(num_output_channels=3),\n",
        "                                            transforms.ToTensor()])\n",
        "\n",
        "transforms_test = transforms.Compose([transforms.ToPILImage(),\n",
        "                                      transforms.Grayscale(num_output_channels=3),\n",
        "                                      transforms.ToTensor()])\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "kW5USHQK34Oo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load data\n",
        "# @markdown create 3 dataloaders to load Training, validation, and test data.\n",
        "# Create the dataset\n",
        "train_dataset = FER2013(csv_file=dataset_csv_file, split='Training', transforms=transforms_train)\n",
        "validation_dataset = FER2013(csv_file=dataset_csv_file, split='PublicTest', transforms=transforms_validation)\n",
        "test_dataset = FER2013(csv_file=dataset_csv_file, split='PrivateTest', transforms=transforms_test)\n",
        "\n",
        "# Create the data loader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "D3DWvoCB4feC",
        "cellView": "form"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot some samples\n",
        "\n"
      ],
      "metadata": {
        "id": "4CekCTBG5JtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plotting methods\n",
        "# @markdown Plot dataset images (before transformation) and dataloader images (after transformation)\n",
        "def show_dataloader_batch(dataloader):\n",
        "  batch = next(iter(dataloader))\n",
        "  # Extract the images and labels from the batch\n",
        "  images, labels = batch\n",
        "  # fig = plt.figure(figsize=(10, 10))\n",
        "  for i in range(len(images)):\n",
        "      ax = plt.subplot(4,len(images)//4 + 1, i+ 1)\n",
        "      image = images[i]\n",
        "      if len(image.shape) == 3:\n",
        "        image = image.permute(1,2,0)\n",
        "      ax.imshow(image, cmap=\"gray\")\n",
        "      ax.set_title(f\"{emotions[labels[i].item()]}\")\n",
        "      ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "def show_dataset_batch(dataset, batch_size=20):\n",
        "  for i in range(batch_size):\n",
        "      image, label = dataset._samples[i]\n",
        "      ax = plt.subplot(4, batch_size//4 + 1, i+ 1)\n",
        "      if len(image.shape) == 3:\n",
        "        image = image.permute(1,2,0)\n",
        "      ax.imshow(image, cmap=\"gray\")\n",
        "      ax.set_title(f\"{emotions[label]}\")\n",
        "      ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yoPJx6fcJzCC",
        "cellView": "form"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Dataset Samples\")\n",
        "show_dataset_batch(train_dataset)\n",
        "print(\"Transformed Train Dataset Samples\")\n",
        "show_dataloader_batch(test_dataloader)"
      ],
      "metadata": {
        "id": "-xvpmj5OOjmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocessing\n",
        "# Face Alignment"
      ],
      "metadata": {
        "id": "xS_sROsF-oyr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n"
      ],
      "metadata": {
        "id": "bCSAhUZs6ZBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained models are models that have been trained on large datasets. By utilizing their learned weights, we can enhance our own models. We can modify or add layers to the pretrained model, enabling us to train a smaller number of parameters during our own training process. This approach helps to leverage existing knowledge and accelerate model training."
      ],
      "metadata": {
        "id": "HrowTfD3-C8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title log method\n",
        "def log(hyperparameters, results, text, csv_file_path):\n",
        "    # Check if CSV file path exists, create it if not\n",
        "    new_file = False\n",
        "    if not os.path.exists(csv_file_path):\n",
        "        new_file = True\n",
        "        open(csv_file_path, 'w').close()\n",
        "\n",
        "    # Combine hyperparameters, results, and text into a single list\n",
        "    data = []\n",
        "    data.extend(hyperparameters.items())\n",
        "    data.extend(results.items())\n",
        "    data.append(('explanation', text))\n",
        "    data.append(('run_date', datetime.now()))\n",
        "\n",
        "    # Append data to CSV file\n",
        "    with open(csv_file_path, 'a', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        if new_file: writer.writerow([key for key, _ in data])\n",
        "        writer.writerow([value for _, value in data])\n",
        "\n",
        "    print(\"Results, hyperparameters, and explanation appended to CSV file:\", csv_file_path)\n"
      ],
      "metadata": {
        "id": "LibQNKGrRf60"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture 01: ResNet"
      ],
      "metadata": {
        "id": "pdJZ3MocTOt7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ResNet (Residual Network) is a deep learning architecture that addresses the challenge of training very deep neural networks. It introduces residual connections, or skip connections, which allow information to flow directly across layers. This helps mitigate the degradation problem, where the performance of deeper networks decreases due to vanishing gradients. ResNet's residual connections enable the training of extremely deep networks, achieving state-of-the-art performance in various computer vision tasks, such as image classification and object detection."
      ],
      "metadata": {
        "id": "Q7-Mnhe88yjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define ResNet model and download pre-trained weights\n",
        "resnet_model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "\n",
        "# Freeze Layers\n",
        "for param in resnet_model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Display model summary\n",
        "summary(resnet_model, input_size=(1, 3, 48, 48))"
      ],
      "metadata": {
        "id": "LGyhw-Q-_l2l",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Once the weights have been downloaded, we proceed to modify specific layers as needed. By doing so, we can observe the impact on the accuracy of both the training and validation data."
      ],
      "metadata": {
        "id": "1XpPEHpd-7dU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 01-01- Changing the last Layer:\n",
        "Since ResNet classifies images in 1000 classes and FER2013 has 7 classes, we changed the last layer."
      ],
      "metadata": {
        "id": "Z-yNHN3r_FBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the last Layer:\n",
        "resnet_model.fc = torch.nn.Linear(in_features=512, out_features=7)\n",
        "resnet_model.softmax = torch.nn.Softmax(dim=1)\n",
        "# Display model summary\n",
        "summary(resnet_model, input_size=(1, 3, 48, 48))"
      ],
      "metadata": {
        "id": "hdqziqLZTvow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hyperparameters\n",
        "base_learning_rate = 1e-4\n",
        "best_acc = 0"
      ],
      "metadata": {
        "id": "6QxNdfNzWz6j",
        "cellView": "form"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Define requiered methods"
      ],
      "metadata": {
        "id": "09BdSnlmBS55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title checkpoint & adjust_learning_rate\n",
        "def checkpoint(model, acc, epoch, out_model_name):\n",
        "  # Save checkpoint.\n",
        "  print('Saving..')\n",
        "  state = {\n",
        "      'state_dict': model.state_dict(),\n",
        "      'acc': acc,\n",
        "      'epoch': epoch,\n",
        "      'rng_state': torch.get_rng_state()\n",
        "  }\n",
        "  if not os.path.isdir('checkpoint'):\n",
        "      os.mkdir('checkpoint')\n",
        "  torch.save(state, f'./checkpoint/{out_model_name}.t7')\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch):\n",
        "  \"\"\"decrease the learning rate at 100 and 150 epoch\"\"\"\n",
        "  lr = base_learning_rate\n",
        "  if epoch <= 9 and lr > 0.1:\n",
        "    # warm-up training for large minibatch\n",
        "    lr = 0.1 + (base_learning_rate - 0.1) * epoch / 10.\n",
        "  if epoch >= 100:\n",
        "    lr /= 10\n",
        "  if epoch >= 150:\n",
        "    lr /= 10\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "EoBesiyAWxB6",
        "cellView": "form"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train & test\n",
        "def train(net, epoch, use_cuda=True, pbar=None):\n",
        "  net.train()\n",
        "  train_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # pbar = tqdm(enumerate(train_dataloader))\n",
        "  for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "    if use_cuda:\n",
        "      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    inputs, targets = Variable(inputs), Variable(targets)\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += targets.size(0)\n",
        "    correct += predicted.eq(targets.data).cpu().sum()\n",
        "    result = f\"Epoch:{epoch} Training-Batch: {batch_idx}/{len(train_dataloader)}| Loss: {(train_loss/(batch_idx+1)):0.3f} | Acc: {(100.*correct/total):0.3f}\"\n",
        "    pbar.set_description(result)\n",
        "  return (train_loss/batch_idx, 100.*correct/total)\n",
        "\n",
        "def test(net, epoch, out_model_name, use_cuda=True, pbar=None):\n",
        "  global best_acc\n",
        "  net.eval()\n",
        "  test_loss, correct, total = 0, 0, 0\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (inputs, targets) in enumerate(validation_dataloader):\n",
        "      if use_cuda:\n",
        "        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, targets)\n",
        "\n",
        "      test_loss += loss.item()\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total += targets.size(0)\n",
        "      correct += predicted.eq(targets.data).cpu().sum()\n",
        "      result = f\"Epoch:{epoch} Testing-Batch: {batch_idx}/{len(validation_dataloader)}| Loss: {(test_loss/(batch_idx+1)):0.3f} | Acc: {(100.*correct/total):0.3f}\"\n",
        "      pbar.set_description(result)\n",
        "\n",
        "  # Save checkpoint.\n",
        "  acc = 100.*correct/total\n",
        "  if acc > best_acc:\n",
        "    best_acc = acc\n",
        "    checkpoint(net, acc, epoch, out_model_name)\n",
        "  return (test_loss/batch_idx, 100.*correct/total)"
      ],
      "metadata": {
        "id": "dA0sjZXhWJpD",
        "cellView": "form"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title History class to save accuracy and loss during the training process\n",
        "class History:\n",
        "  def __init__(self):\n",
        "    self.train_acc = []\n",
        "    self.validation_acc = []\n",
        "    self.test_acc = []\n",
        "    self.train_loss = []\n",
        "    self.validation_loss = []\n",
        "    self.test_loss = []\n",
        "\n",
        "  def append_results(self, loss, acc, split):\n",
        "    if split == \"train\":\n",
        "      self.train_loss.append(loss)\n",
        "      self.train_acc.append(acc)\n",
        "    if split == \"validation\":\n",
        "      self.validation_loss.append(loss)\n",
        "      self.validation_acc.append(acc)\n",
        "    if split == \"test\":\n",
        "      self.test_loss.append(loss)\n",
        "      self.test_acc.append(acc)"
      ],
      "metadata": {
        "id": "QZYAnRw1pKUt",
        "cellView": "form"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot Accuarcy and Loss dusring the trainig process\n",
        "def plot_history(history):\n",
        "  # Plotting\n",
        "  plt.figure(figsize=(10,3))\n",
        "  ax = plt.subplot(1,2,1)\n",
        "  ax.plot(history.train_loss)\n",
        "  ax.plot(history.test_loss)\n",
        "  ax.set_title(\"Loss\")\n",
        "  ax.legend([\"train_loss\", \"test_loss\"])\n",
        "  ax = plt.subplot(1,2,2)\n",
        "  ax.plot(history.train_acc)\n",
        "  ax.plot(history.test_acc)\n",
        "  ax.set_ylim(0,100)\n",
        "  ax.set_title(\"Accuracy\")\n",
        "  ax.legend([\"train_acc\", \"test_acc\"])\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jGDFfHRv3-0Z",
        "cellView": "form"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Train the model"
      ],
      "metadata": {
        "id": "MOAH3PdlXAeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set loss function and optimizer\n",
        "# Optimizer and criterion\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(resnet_model.parameters(), lr=base_learning_rate, momentum=0.9, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "dDQaGeImVZ42",
        "cellView": "form"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training\n",
        "start_epoch = 0\n",
        "max_epochs = 2\n",
        "\n",
        "out_model_name = \"resnet18\"\n",
        "history = History()\n",
        "pbar = tqdm(range(start_epoch, max_epochs))\n",
        "for epoch in pbar:\n",
        "  # adjust_learning_rate(optimizer, epoch)\n",
        "  train_loss, train_acc = train(resnet_model, epoch, use_cuda=use_cuda, pbar=pbar)\n",
        "  history.append_results(train_loss, train_acc, \"train\")\n",
        "  validation_loss, validation_acc = test(resnet_model, epoch, out_model_name, use_cuda=use_cuda, pbar=pbar)\n",
        "  history.append_results(validation_loss, validation_acc, \"test\")\n",
        "  print(f'  Epoch:{epoch + 1} | Train acc: {train_acc} | validationt acc: {validation_acc}')"
      ],
      "metadata": {
        "id": "ZplRtJ6oW_vN",
        "outputId": "faea1171-3c73-47de-f154-4db52912c9a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:0 Testing-Batch: 112/113| Loss: 1.843 | Acc: 26.999:  50%|█████     | 1/2 [04:09<04:09, 249.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving..\n",
            "  Epoch:1 | Train acc: 23.456058502197266 | validationt acc: 26.999164581298828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:1 Testing-Batch: 112/113| Loss: 1.784 | Acc: 29.200: 100%|██████████| 2/2 [08:14<00:00, 247.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving..\n",
            "  Epoch:2 | Train acc: 27.95290756225586 | validationt acc: 29.200334548950195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save this training in log file and save the model.\n",
        "all_hparam = {\n",
        "    'learning_rate': base_learning_rate,\n",
        "    'batch_size': batch_size,\n",
        "    'num_epochs': max_epochs,\n",
        "    'optimizer' : optimizer,\n",
        "    'loss_function' : criterion,\n",
        "}\n",
        "model_results = {\n",
        "    'model' : summary(resnet_model, input_size=(1, 3, 48, 48)),\n",
        "    'mean_train_loss' : np.mean(history.train_loss),\n",
        "    'mean_train_acc' : np.mean(history.train_acc),\n",
        "    'std_train_acc' : np.std(history.train_acc),\n",
        "    'mean_validation_loss': np.mean(history.validation_loss),\n",
        "    'mean_validation_acc' : np.mean(history.validation_acc),\n",
        "    'std_validation_acc' : np.std(history.train_acc),\n",
        "}\n",
        "explain_text = \"Just add a layer\"\n",
        "log(all_hparam,model_results,explain_text,log_file_path)\n",
        "\n",
        "model_path = os.path.join(model_directory,\"model\"+str(datetime.now())+\".pt\")\n",
        "torch.save(resnet_model, model_path)"
      ],
      "metadata": {
        "id": "J1ZUufvyCeT8",
        "outputId": "a5e95afb-89ee-4e2e-ca75-be802b562852",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results, hyperparameters, and explanation appended to CSV file: /content/drive/MyDrive/Colab Notebooks/Logs/log_models.csv\n"
          ]
        }
      ]
    }
  ]
}